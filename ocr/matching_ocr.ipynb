{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yogender-Singh/Notebooks/blob/main/ocr/matching_ocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFhAzQB3aNMa"
      },
      "source": [
        "### Download the code zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0LG1EuaRlB"
      },
      "source": [
        "!wget qq https://github.com/Yogender-Singh/Notebooks/raw/main/images/matching-ocr.zip\n",
        "!unzip -qq matching-ocr.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrOk6pURp50"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJaCNlDDRz6d"
      },
      "source": [
        "# import the necessary packages\n",
        "from imutils import contours\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBrLwCtN5kqy"
      },
      "source": [
        "### Function to display images in Jupyter Notebooks and Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRw969Dp5Kdm"
      },
      "source": [
        "def plt_imshow(title, image):\n",
        "  # convert the image frame BGR to RGB color space and display it\n",
        "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\tplt.imshow(image)\n",
        "\tplt.title(title)\n",
        "\tplt.grid(False)\n",
        "\tplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9Cs7euehjlF"
      },
      "source": [
        "### OCR via template matching with OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWs-qK1whJr0"
      },
      "source": [
        "# # construct the argument parser and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "# \thelp=\"path to input image\")\n",
        "# ap.add_argument(\"-r\", \"--reference\", required=True,\n",
        "# \thelp=\"path to reference OCR-A image\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "\t\"image\": \"images/credit_card_01.png\",\n",
        "\t\"reference\": \"ocr_a_reference.png\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzhGbfbJiH5l"
      },
      "source": [
        "# define a dictionary that maps the first digit of a credit card\n",
        "# number to the credit card type\n",
        "FIRST_NUMBER = {\n",
        "\t\"3\": \"American Express\",\n",
        "\t\"4\": \"Visa\",\n",
        "\t\"5\": \"MasterCard\",\n",
        "\t\"6\": \"Discover Card\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCFPhUlViKtg"
      },
      "source": [
        "# load the reference OCR-A image from disk, convert it to grayscale,\n",
        "# and threshold it, such that the digits appear as *white* on a\n",
        "# *black* background\n",
        "# and invert it, such that the digits appear as *white* on a *black*\n",
        "ref = cv2.imread(args[\"reference\"])\n",
        "ref = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY)\n",
        "ref = cv2.threshold(ref, 10, 255, cv2.THRESH_BINARY_INV)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPsV-p3wiM7Q"
      },
      "source": [
        "# find contours in the OCR-A image (i.e,. the outlines of the digits)\n",
        "# sort them from left to right, and initialize a dictionary to map\n",
        "# digit name to the ROI\n",
        "refCnts = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,\n",
        "\tcv2.CHAIN_APPROX_SIMPLE)\n",
        "refCnts = imutils.grab_contours(refCnts)\n",
        "refCnts = contours.sort_contours(refCnts, method=\"left-to-right\")[0]\n",
        "digits = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8diG8e7yicks"
      },
      "source": [
        "# loop over the OCR-A reference contours\n",
        "for (i, c) in enumerate(refCnts):\n",
        "\t# compute the bounding box for the digit, extract it, and resize\n",
        "\t# it to a fixed size\n",
        "\t(x, y, w, h) = cv2.boundingRect(c)\n",
        "\troi = ref[y:y + h, x:x + w]\n",
        "\troi = cv2.resize(roi, (57, 88))\n",
        "\n",
        "\t# update the digits dictionary, mapping the digit name to the ROI\n",
        "\tdigits[i] = roi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6E56l9Kih9r"
      },
      "source": [
        "# initialize a rectangular (wider than it is tall) and square\n",
        "# structuring kernel\n",
        "rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 3))\n",
        "sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRF0V48BikZj"
      },
      "source": [
        "# load the input image, resize it, and convert it to grayscale\n",
        "image = cv2.imread(args[\"image\"])\n",
        "image = imutils.resize(image, width=300)\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPqWe6OAindD"
      },
      "source": [
        "# apply a tophat (whitehat) morphological operator to find light\n",
        "# regions against a dark background (i.e., the credit card numbers)\n",
        "tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, rectKernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YcG6_Xmiqrb"
      },
      "source": [
        "# compute the Scharr gradient of the tophat image, then scale\n",
        "# the rest back into the range [0, 255]\n",
        "gradX = cv2.Sobel(tophat, ddepth=cv2.CV_32F, dx=1, dy=0,\n",
        "\tksize=-1)\n",
        "gradX = np.absolute(gradX)\n",
        "(minVal, maxVal) = (np.min(gradX), np.max(gradX))\n",
        "gradX = (255 * ((gradX - minVal) / (maxVal - minVal)))\n",
        "gradX = gradX.astype(\"uint8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnTtZK3aisy0"
      },
      "source": [
        "# apply a closing operation using the rectangular kernel to help\n",
        "# cloes gaps in between credit card number digits, then apply\n",
        "# Otsu's thresholding method to binarize the image\n",
        "gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel)\n",
        "thresh = cv2.threshold(gradX, 0, 255,\n",
        "\tcv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "    \n",
        "# apply a second closing operation to the binary image, again\n",
        "# to help close gaps between credit card number regions\n",
        "thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQVG1IRLixHV"
      },
      "source": [
        "# find contours in the thresholded image, then initialize the\n",
        "# list of digit locations\n",
        "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
        "\tcv2.CHAIN_APPROX_SIMPLE)\n",
        "cnts = imutils.grab_contours(cnts)\n",
        "locs = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hanahWStizf1"
      },
      "source": [
        "# loop over the contours\n",
        "for (i, c) in enumerate(cnts):\n",
        "\t# compute the bounding box of the contour, then use the\n",
        "\t# bounding box coordinates to derive the aspect ratio\n",
        "\t(x, y, w, h) = cv2.boundingRect(c)\n",
        "\tar = w / float(h)\n",
        "\n",
        "\t# since credit cards used a fixed size fonts with 4 groups\n",
        "\t# of 4 digits, we can prune potential contours based on the\n",
        "\t# aspect ratio\n",
        "\tif ar > 2.5 and ar < 4.0:\n",
        "\t\t# contours can further be pruned on minimum/maximum width\n",
        "\t\t# and height\n",
        "\t\tif (w > 40 and w < 55) and (h > 10 and h < 20):\n",
        "\t\t\t# append the bounding box region of the digits group\n",
        "\t\t\t# to our locations list\n",
        "\t\t\tlocs.append((x, y, w, h))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rHAC95Si4Ku"
      },
      "source": [
        "# sort the digit locations from left-to-right, then initialize the\n",
        "# list of classified digits\n",
        "locs = sorted(locs, key=lambda x:x[0])\n",
        "output = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc7gKYVOi6ue"
      },
      "source": [
        "# loop over the 4 groupings of 4 digits\n",
        "for (i, (gX, gY, gW, gH)) in enumerate(locs):\n",
        "\t# initialize the list of group digits\n",
        "\tgroupOutput = []\n",
        "\n",
        "\t# extract the group ROI of 4 digits from the grayscale image,\n",
        "\t# then apply thresholding to segment the digits from the\n",
        "\t# background of the credit card\n",
        "\tgroup = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5]\n",
        "\tgroup = cv2.threshold(group, 0, 255,\n",
        "\t\tcv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "\t# detect the contours of each individual digit in the group,\n",
        "\t# then sort the digit contours from left to right\n",
        "\tdigitCnts = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL,\n",
        "\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
        "\tdigitCnts = imutils.grab_contours(digitCnts)\n",
        "\tdigitCnts = contours.sort_contours(digitCnts,\n",
        "\t\tmethod=\"left-to-right\")[0]\n",
        "\n",
        "\t# loop over the digit contours\n",
        "\tfor c in digitCnts:\n",
        "\t\t# compute the bounding box of the individual digit, extract\n",
        "\t\t# the digit, and resize it to have the same fixed size as\n",
        "\t\t# the reference OCR-A images\n",
        "\t\t(x, y, w, h) = cv2.boundingRect(c)\n",
        "\t\troi = group[y:y + h, x:x + w]\n",
        "\t\troi = cv2.resize(roi, (57, 88))\n",
        "\n",
        "\t\t# initialize a list of template matching scores\n",
        "\t\tscores = []\n",
        "\n",
        "\t\t# loop over the reference digit name and digit ROI\n",
        "\t\tfor (digit, digitROI) in digits.items():\n",
        "\t\t\t# apply correlation-based template matching, take the\n",
        "\t\t\t# score, and update the scores list\n",
        "\t\t\tresult = cv2.matchTemplate(roi, digitROI,\n",
        "\t\t\t\tcv2.TM_CCOEFF)\n",
        "\t\t\t(_, score, _, _) = cv2.minMaxLoc(result)\n",
        "\t\t\tscores.append(score)\n",
        "\n",
        "\t\t# the classification for the digit ROI will be the reference\n",
        "\t\t# digit name with the *largest* template matching score\n",
        "\t\tgroupOutput.append(str(np.argmax(scores)))\n",
        "\n",
        "\t# draw the digit classifications around the group\n",
        "\tcv2.rectangle(image, (gX - 5, gY - 5),\n",
        "\t\t(gX + gW + 5, gY + gH + 5), (0, 0, 255), 2)\n",
        "\tcv2.putText(image, \"\".join(groupOutput), (gX, gY - 15),\n",
        "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2)\n",
        "\n",
        "\t# update the output digits list\n",
        "\toutput.extend(groupOutput)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsA0tAz9jBQH"
      },
      "source": [
        "# display the output credit card information to the screen\n",
        "print(\"Credit Card Type: {}\".format(FIRST_NUMBER[output[0]]))\n",
        "print(\"Credit Card #: {}\".format(\"\".join(output)))\n",
        "plt_imshow(\"Image\", image)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}